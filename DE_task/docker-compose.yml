x-postgresql-connection-env: &pg-connect
  POSTGRESQL_APP_HOST: ${POSTGRESQL_APP_HOST}
  POSTGRESQL_APP_DB: ${POSTGRESQL_APP_DB}
  POSTGRESQL_APP_SCHEMA: ${POSTGRESQL_APP_SCHEMA}
  POSTGRESQL_APP_USER: ${POSTGRESQL_APP_USER}
  POSTGRESQL_APP_PASSWORD: ${POSTGRESQL_APP_PASSWORD}

x-mysql-connection-env: &mysql-connect
  MYSQL_APP_HOST: ${MYSQL_APP_HOST}
  MYSQL_APP_DB: ${MYSQL_APP_DB}
  MYSQL_APP_USER: ${MYSQL_APP_USER}
  MYSQL_APP_PASSWORD: ${MYSQL_APP_PASSWORD}

x-airflow-common-env: &airflow-common-env
  AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW__CORE__LOAD_EXAMPLES}
  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
  AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: ${AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK}
  AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW__WEBSERVER__SECRET_KEY}
  AIRFLOW_UID: ${AIRFLOW_UID}
  _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME}
  _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD}
  SPARK_MASTER_HOST: ${SPARK_MASTER_HOST}
  SPARK_MASTER_PORT: ${SPARK_MASTER_PORT}

x-spark-common-env: &spark-common-env
  SPARK_RPC_AUTHENTICATION_ENABLED: ${SPARK_RPC_AUTHENTICATION_ENABLED}
  SPARK_RPC_ENCRYPTION_ENABLED: ${SPARK_RPC_ENCRYPTION_ENABLED}
  SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: ${SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED}
  SPARK_SSL_ENABLED: ${SPARK_SSL_ENABLED}
services:
  mysql:
    build: ./db/mysql
    container_name: mysql
    # hostname: ${MYSQL_APP_HOST}
    ports:
      - "3306:3306"
    volumes:
      - mysql_data:/var/lib/mysql
    environment:
      <<: *mysql-connect
      # Пароль от root
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
    env_file:
      - .env

    healthcheck:
      test: ["CMD-SHELL", "mysqladmin ping -h localhost"]
      interval: 5s
      timeout: 5s
      retries: 5

    restart: always

  postgres:
    build: ./db/postgresql
    container_name: postgres
    # hostname: ${POSTGRESQL_APP_HOST}
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      <<: *pg-connect
      # Пароль от root
      POSTGRES_PASSWORD: ${POSTGRESQL_ROOT_PASSWORD}
    env_file:
      - .env
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5

    restart: always

  datagen:
    build: ./datagen
    container_name: pg_datagen
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      <<: *pg-connect
      AI_USER_ID: ${AI_USER_ID}
      AI_API_KEY: ${AI_API_KEY}
    restart: no
  
  spark-master:
    build: ./spark/spark-master
    container_name: spark-master
    environment:
      <<: *spark-common-env
      SPARK_MODE: master
      SPARK_MASTER_WEBUI_PORT: 8081
    ports:
      - "8081:8081"
    volumes:
      - spark_data:/bitnami
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8081"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G

  spark-worker:
    build: ./spark/spark-worker
    container_name: spark-worker
    environment:
      <<: *spark-common-env
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://${SPARK_MASTER_HOST}:${SPARK_MASTER_PORT}
      SPARK_WORKER_MEMORY: 1G
      SPARK_WORKER_CORES: 1
    volumes:
      - spark_data:/bitnami
    depends_on:
      spark-master:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G

  airflow-init:
    build: ./airflow/init
    container_name: airflow-init
    depends_on:
      postgres:
        condition: service_healthy
      mysql:
        condition: service_healthy
      spark-master:
        condition: service_healthy
      datagen:
        condition: service_completed_successfully
    environment:
      <<: [*airflow-common-env, *pg-connect, *mysql-connect]
    volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/pyspark_jobs:/opt/airflow/scripts
    - sqlite_airflow_data:/usr/local/airflow/db:rw
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  airflow-scheduler:
    build: ./airflow/scheduler
    container_name: airflow-scheduler
    environment:
      <<: [*airflow-common-env]
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/pyspark_jobs:/opt/airflow/scripts
    - sqlite_airflow_data:/usr/local/airflow/db:rw
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G

  airflow-webserver:
    build: ./airflow/webserver
    container_name: airflow-webserver
    environment:
      <<: *airflow-common-env
    ports:
      - "8080:8080"
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    volumes:
    - .airflow/dags:/opt/airflow/dags
    - .airflow/pyspark_jobs:/opt/airflow/scripts
    - sqlite_airflow_data:/usr/local/airflow/db:rw
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G

volumes:
  postgres_data:
  mysql_data:
  sqlite_airflow_data:
  spark_data: